{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945874ce11294293b0f67e8649178047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9777b0dad44ea184ee5487f9371cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ea51ce923546068947fedddb97cac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b5880ab7804b0eab1f5840e38b2de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff0141fcf174041a9623412820e9cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9978cc83ed4e859df9ddd38bfc3264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532d479a23b04b848171b58a43d1cb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Download the pretrained model and tokenizer for French to English translation\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n"
     ]
    }
   ],
   "source": [
    "def translate_text(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Translate the input text\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    # Decode the translated output\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "translated_text = translate_text(input_text, model, tokenizer)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, how are you?', \"What's your name?\"]\n"
     ]
    }
   ],
   "source": [
    "def translate_batch(texts, model, tokenizer):\n",
    "    # Tokenize the input texts\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Translate the input texts\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    # Decode the translated outputs\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "input_texts = [\"Bonjour, comment ça va?\", \"Quel est ton nom?\"]\n",
    "translated_texts = translate_batch(input_texts, model, tokenizer)\n",
    "print(translated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size=512):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for token in text.split():\n",
    "        if len(current_chunk) + len(token) + 1 <= max_chunk_size:\n",
    "            current_chunk += token + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = token + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_chunks_in_batches(chunks, batch_size=4):\n",
    "    translated_texts = []\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        batch_input = \" \".join(batch)\n",
    "        translated_batch =translate_batch(batch_input,model, tokenizer)\n",
    "        translated_texts.extend(translated_batch.split(\"\\n\"))\n",
    "\n",
    "    return translated_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_fr_translate_long_text(text, max_chunk_size=512, batch_size=4):\n",
    "    chunks = chunk_text(text, max_chunk_size)\n",
    "    translated_chunks = translate_chunks_in_batches(chunks, batch_size)\n",
    "    translated_text = \" \".join(translated_chunks)\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m long_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYour very long input text...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m translated_text \u001b[39m=\u001b[39m en_fr_translate_long_text(long_text)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(translated_text)\n",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36men_fr_translate_long_text\u001b[1;34m(text, max_chunk_size, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39men_fr_translate_long_text\u001b[39m(text, max_chunk_size\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m):\n\u001b[0;32m      2\u001b[0m     chunks \u001b[39m=\u001b[39m chunk_text(text, max_chunk_size)\n\u001b[1;32m----> 3\u001b[0m     translated_chunks \u001b[39m=\u001b[39m translate_chunks_in_batches(chunks, batch_size)\n\u001b[0;32m      4\u001b[0m     translated_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(translated_chunks)\n\u001b[0;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m translated_text\n",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtranslate_chunks_in_batches\u001b[1;34m(chunks, batch_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m     batch_input \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(batch)\n\u001b[0;32m      7\u001b[0m     translated_batch \u001b[39m=\u001b[39mtranslate_batch(batch_input,model, tokenizer)\n\u001b[1;32m----> 8\u001b[0m     translated_texts\u001b[39m.\u001b[39mextend(translated_batch\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m translated_texts\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "long_text = \"Your very long input text...\"\n",
    "translated_text = en_fr_translate_long_text(long_text)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
