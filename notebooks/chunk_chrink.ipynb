{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import RegexpParser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_financial=pd.read_csv('../financial_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data_financial['text'][2]\n",
    "\n",
    "'''part-of-speech (POS) tagging to identify the grammatical roles of each word'''\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Identify noun phrases in the text'''\n",
    "\n",
    "chunk_grammar = r\"\"\"\n",
    "  NP: {<DT>?<JJ>*<NN.*>+}   # chunk noun phrases\n",
    "\"\"\"\n",
    "cp = RegexpParser(chunk_grammar)\n",
    "chunked = cp.parse(tagged)\n",
    "    \n",
    "'''exclude prepositions and conjunctions from noun phrases'''\n",
    "\n",
    "chink_grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}                # match any word\n",
    "    }<IN|CC>+{             # chink prepositions and conjunctions\n",
    "\"\"\"\n",
    "cp = RegexpParser(chink_grammar)\n",
    "chinked = cp.parse(chunked)\\\n",
    "\n",
    "\n",
    "chunked.draw()\n",
    "chinked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic chuncking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rimai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda(tokenized_sentences, num_topics=5, num_passes=10):\n",
    "    dictionary = Dictionary(tokenized_sentences)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_sentences]\n",
    "\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        passes=num_passes,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topics(lda_model, tokenized_sentences):\n",
    "    topics_per_sentence = []\n",
    "\n",
    "    for sent_tokens in tokenized_sentences:\n",
    "        bow_vector = lda_model.id2word.doc2bow(sent_tokens)\n",
    "        topic_probs = lda_model.get_document_topics(bow_vector)\n",
    "        topics = [topic for topic, prob in topic_probs]\n",
    "        topics_per_sentence.append(topics)\n",
    "\n",
    "    return topics_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_based_chunking(tokenized_sentences, topics_per_sentence):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    prev_topics = []\n",
    "\n",
    "    for sent_tokens, topics in zip(tokenized_sentences, topics_per_sentence):\n",
    "        if not prev_topics:\n",
    "            current_chunk.append(sent_tokens)\n",
    "            prev_topics = topics\n",
    "        elif set(prev_topics) == set(topics):\n",
    "            current_chunk.append(sent_tokens)\n",
    "            prev_topics = topics\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [sent_tokens]\n",
    "            prev_topics = topics\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../extracted_data/01 01 2023_Goldman Sachs_Caution Heavy Fog.txt', 'r',encoding='utf-8') as file:\n",
    "    text= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2344"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model = perform_lda(tokenized_sentences, num_topics=5, num_passes=10)\n",
    "topics_per_sentence = assign_topics(lda_model, tokenized_sentences)\n",
    "chunks = topic_based_chunking(tokenized_sentences, topics_per_sentence)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    ch=(\" \".join(sent_tokens) for sent_tokens in chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_based_chunking(text, max_chunk_words=200):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_word_count = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        sent_word_count = len(words)\n",
    "\n",
    "        if current_chunk_word_count + sent_word_count <= max_chunk_words:\n",
    "            current_chunk.append(sent)\n",
    "            current_chunk_word_count += sent_word_count\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = [sent]\n",
    "            current_chunk_word_count = sent_word_count\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "max_chunk_words = 200\n",
    "chunks = sentence_based_chunking(text, max_chunk_words)\n",
    "print(type(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The maximum \\ndrawdown in the price of 10-year US Treasuries was 22%.',\n",
       " 'It seldom happens \\nthat 10-year Treasuries—which have half the volatility of US equities—drop \\nnearly as much as these equities.',\n",
       " 'The maximum drawdown of 10-year German bunds was 21%, and that of \\n10-year UK gilts, 26%.',\n",
       " 'This drop in the price of 10-year UK gilts stood in sharp \\ncontrast to the 5% total return of UK equities, as measured by the FTSE 100.',\n",
       " 'The latter’s positive return was driven primarily by the energy sector and the \\nmetals and mining sector.',\n",
       " 'Shorter-term bond benchmarks declined less but declined nonetheless: the \\nmaximum drop was 11% for 1- to 10- year US Treasuries, 12% for 1- to 10-year \\nGerman bunds and 15% for UK gilts.',\n",
       " 'Oil and natural gas were thrown into turmoil with the invasion of Ukraine, \\nand energy security has been thrust back into the forefront after a long hiatus.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A recession in the \\nworld’s largest economy would reverberate globally. The US Congress may repeat \\nthe mistakes of 2011 in failing to raise the debt limit in a timely and orderly \\nmanner. China’s disorderly abandonment of its “zero-COVID” policy may \\nunleash another wave of COVID-19 infections, including new variants, globally. The geopolitical outlook for 2023, too, is foggy and fraught with risk. There is \\nno face-saving off-ramp for Russia from Ukraine. China is unlikely to reverse its \\nassertive and aggressive posture. North Korea is expected to continue, even step \\nup, its ballistic missile tests. Iran may proceed to enrich its uranium to weapons-\\ngrade levels, which could elicit a military response by Israel. We proceed with caution. We start with a careful review of the turmoil in financial markets last year \\nbecause this backdrop is important in understanding the fog of uncertainty still \\nfacing investors. 2\\nGoldman Sachs\\njanuary 2023\\n\\n \\nAmong financial assets, none were spared in 2022.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separator=\" \"\n",
    "separator.join(chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'sentence':\n",
      "[ 9.5566749e-05  3.0766914e-03 -6.8137394e-03 -1.3765570e-03\n",
      "  7.6686372e-03  7.3468201e-03 -3.6746713e-03  2.6434178e-03\n",
      " -8.3172321e-03  6.2056510e-03 -4.6381601e-03 -3.1638264e-03\n",
      "  9.3112551e-03  8.7383972e-04  7.4904407e-03 -6.0747396e-03\n",
      "  5.1616658e-03  9.9239927e-03 -8.4574483e-03 -5.1358812e-03\n",
      " -7.0656319e-03 -4.8641083e-03 -3.7799017e-03 -8.5363444e-03\n",
      "  7.9560187e-03 -4.8446450e-03  8.4246909e-03  5.2640396e-03\n",
      " -6.5507693e-03  3.9591030e-03  5.4700258e-03 -7.4266563e-03\n",
      " -7.4070138e-03 -2.4764915e-03 -8.6265476e-03 -1.5821959e-03\n",
      " -4.0328439e-04  3.2992417e-03  1.4416119e-03 -8.8057260e-04\n",
      " -5.5943532e-03  1.7301576e-03 -8.9744868e-04  6.7944760e-03\n",
      "  3.9740861e-03  4.5300853e-03  1.4343826e-03 -2.7005500e-03\n",
      " -4.3667960e-03 -1.0317774e-03  1.4371726e-03 -2.6462940e-03\n",
      " -7.0744529e-03 -7.8055691e-03 -9.1231214e-03 -5.9355209e-03\n",
      " -1.8478223e-03 -4.3243608e-03 -6.4619179e-03 -3.7169741e-03\n",
      "  4.2902827e-03 -3.7389426e-03  8.3780112e-03  1.5343077e-03\n",
      " -7.2420458e-03  9.4347717e-03  7.6324157e-03  5.4931934e-03\n",
      " -6.8485173e-03  5.8229486e-03  4.0104426e-03  5.1857438e-03\n",
      "  4.2556929e-03  1.9391757e-03 -3.1696800e-03  8.3552850e-03\n",
      "  9.6138241e-03  3.7937930e-03 -2.8362856e-03  7.3486967e-06\n",
      "  1.2190237e-03 -8.4583946e-03 -8.2254484e-03 -2.3115428e-04\n",
      "  1.2368031e-03 -5.7440917e-03 -4.7258148e-03 -7.3460243e-03\n",
      "  8.3296513e-03  1.2086140e-04 -4.5093098e-03  5.7021421e-03\n",
      "  9.1806082e-03 -4.1001113e-03  7.9644257e-03  5.3750905e-03\n",
      "  5.8794441e-03  5.1190186e-04  8.2132714e-03 -7.0191440e-03]\n",
      "\n",
      "Similar words to 'sentence':\n",
      "[(\"'s\", 0.19902050495147705), ('first', 0.172727569937706), (',', 0.17012067139148712), ('phrase', 0.1459527164697647), ('finally', 0.06409681588411331), ('is', 0.046531371772289276), ('second', -0.0027268752455711365), ('third', -0.013542694970965385), ('the', -0.023675668984651566), ('here', -0.03251923993229866)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rimai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample dataset\n",
    "dataset = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"And here's the second phrase.\",\n",
    "    \"Finally, the third sentence.\"\n",
    "]\n",
    "\n",
    "# Tokenize the dataset into words\n",
    "tokenized_dataset = [nltk.word_tokenize(sentence.lower()) for sentence in dataset]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access the word embeddings\n",
    "word_embeddings = model.wv\n",
    "\n",
    "# Get the vector representation of a specific word\n",
    "word_vector = word_embeddings['sentence']\n",
    "\n",
    "# Get similar words to a specific word\n",
    "similar_words = word_embeddings.most_similar('sentence')\n",
    "\n",
    "print(\"Word Vector for 'sentence':\")\n",
    "print(word_vector)\n",
    "\n",
    "print(\"\\nSimilar words to 'sentence':\")\n",
    "print(similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
